\documentclass[titlepage]{ltjsreport}
\usepackage{luatexja, graphicx, multirow, listings, url, hyperref, comment}
\makeatletter
\renewcommand{\abstractname}{アブストラクト}
\newcommand{\image}[2]{\begin{figure}[h]\vspace{1em}\begin{center}\includegraphics[width = .9\textwidth]{#2}\caption{#1}\label{#1}\end{center}\end{figure}}
\newenvironment{mytable}[2]{\begin{table}[h]\begin{center}\caption{#1}\label{#1}\begin{tabular}{#2}}{\end{tabular}\end{center}\end{table}}
\newcommand{\inputtextbox}[3][]{\vspace{1em}\lstset{language = #1}\lstinputlisting[caption = #2, label = #2]{#3}}
\lstnewenvironment{textbox}[2][]{\vspace{1em}\lstset{language = #1, label = #2, caption = #2}}{}
\renewcommand{\lstlistingname}{\figurename}
\makeatother
\lstset{language = , commentstyle = {\itshape}, stringstyle = {\ttfamily}, frame = {tblr}, breaklines = true, numbers = left, xleftmargin = 5.5em, framexleftmargin = 2.5em, numberstyle = {\scriptsize}, captionpos = b}
\begin{document}
\makeatletter
\let\c@lstlisting=\c@figure
\makeatother
\begin{titlepage}
\vspace*{1em}
\begin{center}
\begin{huge}
情報工学特論　課題１
\\
\vspace{1em}
バケツリレーアルゴリズムによる部屋割りの最適化
\end{huge}
\end{center}
\vspace{6em}
\begin{flushright}
\begin{LARGE}
情報工学科　年　番
\\
\vspace{1em}
名前
\end{LARGE}
\end{flushright}
\vspace{4em}
\begin{flushleft}
\begin{Large}
\begin{tabular}{ll}
提出期限： & 2023/12/12（火）23:59
\\
提出日： & 2023/12/12（火）
\\
\end{tabular}
\\
\vspace{2em}
共同実験者：８班
\\
\vspace{1em}
\begin{tabular}{ll}
出席番号 & 名前
\\
出席番号 & 名前
\\
\end{tabular}
\end{Large}
\end{flushleft}
\end{titlepage}
\chapter{問題提起}
修学旅行などで部屋を決める際に全員に配慮することは難しい。そこで８班は強化学習を用いた部屋割りの最適化を行った。まず部屋割りの定義から説明する。「部屋割り」とはいくつかある部屋に人を割り当てることである。この中での「最適化」とは同じ部屋に割り当てられた人同士の相性ができるだけ良い部屋割りを見つけることであり，今回の目的は部屋割りの最適化である。
\section{定式化}
本実験での定義を以下に示す。
\begin{itemize}
\item ３人×４部屋
\item 各人は他人に対して「同室希望」「同室拒否」を設定できる
\item 各人に対して「幸福度」を定義
\begin{itemize}
\item 「同室希望」とした人が同じ部屋→１加算
\item 「同室拒否」とした人が同じ部屋→１減算
\end{itemize}
\item 幸福度の総和が大きいと「良い」部屋割り
\end{itemize}
\chapter{解決方法}
\label{解決方法}
環境は\cite{環境}のEnvironmentクラスに実装されている。

環境は12人が４部屋へどのように割り当てられているかを保持しているが，エージェントはそれを直接観測する訳ではない。

エージェントが観測する状態や環境が受け取る行動を定義するために，「attended rooms」「movable people」という概念を導入する。「attended rooms」とは環境が４部屋のうち２部屋をランダムに選択したものである。「movable people」とはattended roomsから１人ずつ，計２人ランダムに選択したものである。attended roomsには順序の概念があり，それぞれattended room 1・attended room 2とする。movable peopleも同様に，attended room 1から選ばれたmovable person 1とattended room 2から選ばれたmovable person 2の２人からなる。attended rooms・movable peopleを抽選する処理はEnvironmentクラスの\#drawメソッドに実装されている。\#drawメソッドを図\ref{attended rooms・movable peopleの抽選}に示す。この中で使われているsnatchRandomElement関数は引数のSetからランダムに１つ要素を取り出し，その要素を元のSetから削除する関数である。attended rooms・movable peopleの抽選はエージェントが環境に対して行動する度に行われる。
\begin{textbox}{attended rooms・movable peopleの抽選}
#draw() {
	const snatchedRooms = new Set([...this.rooms]);
	this.#attendedRooms = [];
	for (let i = 0; i < this.#a; i++) {
		this.#attendedRooms.push(snatchRandomElement(snatchedRooms));
	}
	this.#movablePeople = [];
	for (const attendedRoom of this.#attendedRooms) {
		const snatchedMembers = new Set([...attendedRoom.members]);
		for (let i = 0; i < this.#m; i++) {
			this.#movablePeople.push(snatchRandomElement(snatchedMembers));
		}
	}
}
\end{textbox}

エージェントが環境に対して状態を要求したとき，環境はmovable personに対して「同室希望」とした人の人数，「同室拒否」とした人の人数をattended room単位で集計する。例えば，movable person 1に対して「同室希望」とした人がattended room 2に２人，「同室拒否」とした人がattended room 2に１人いる場合，attended room 2からmovable person 1に対する集計結果は「同室希望：２　同室拒否：１」となる。環境がエージェントに対して返す状態は以下の４つを２×２の２次元配列に並べたものとなる。
\begin{enumerate}
\item attended room 1からmovable person 1に対する集計結果
\item attended room 2からmovable person 1に対する集計結果
\item attended room 1からmovable person 2に対する集計結果
\item attended room 2からmovable person 2に対する集計結果
\end{enumerate}

環境がエージェントに状態を提供する処理はEnvironmentクラスのgetStateメソッドに実装されている。getStateメソッドを図\ref{状態の提供}に示す。この中で使われているImpressionStatクラスはattended roomからmovable personに対する集計結果を表し，「同室希望」とした人数，「同室拒否」とした人数をそれぞれnumber型でフィールドに持つ。
\begin{textbox}{状態の提供}
getState(): ImpressionStat[][] {
	return this.#movablePeople.map((movablePerson) => {
		return this.#attendedRooms.map(function (attendedRoom) {
			return new ImpressionStat(
			[...attendedRoom.members].filter(function (member) {
				return member.likedPeople.has(movablePerson);
			}).length,
			[...attendedRoom.members].filter(function (member) {
				return member.dislikedPeople.has(movablePerson);
			}).length,
			);
		});
	});
}
\end{textbox}

エージェントが環境に対して行える行動はmovable peopleどうしの入れ替え，あるいは「何もしない」である。\cite{環境}には行動を表すActionクラスが定義されていて，１または２を代入可能なnumber型フィールドを２つ持つ。１・２あるいは２・１を持つAction型インスタンスはmovable person 1とmovable person2の入れ替えを表し，１・１，あるいは２・２を持つインスタンスは「何もしない」を表す。「何もしない」という行動を行った場合も，先述したattended rooms・movable peopleの抽選は行われる。

エージェントが環境に対して行動を行ったとき，幸福度の総和が変化するときがある。行動に対して環境がエージェントに返す報酬は，行動後の幸福度総和から行動前の幸福度総和を引いたものである。
\chapter{アルゴリズム}
僕は，\ref{解決方法}で定義した環境に適応するエージェントをバケツリレーアルゴリズムで学習させた。以下でバケツリレーアルゴリズムについて説明する。

まず，「この状態のときはこの行動をする」というものを「ルール」と定義する。バケツリレーアルゴリズムでは各ルールに価値を設定・更新していく。

エージェントが状態sにあるとき，行動選択手法により行動aを選択し実行した（「状態がsのときはaという行動をする」というルールが「適用された」と呼ぶことにする）場面を考える。このとき「状態がsのときはaという行動をする」の価値のうち一定割合（賭け率）を１ステップ前に適用されたルールの価値に移動させる。次に，行動aにより獲得した報酬を「状態がsのときはaという行動をする」の価値に加える。

価値の更新ができたら，状態の観測と行動選択を行って同様の更新を行う。このような繰り返しにより環境からもたらされた報酬を，報酬を得る過程で適用されたルールへ移動させていくのである。
\chapter{実験内容}
学習進捗データと部屋割り進捗データの２種類のデータを取る。まず学習進捗データとは各エピソードが終了したときに環境からの評価関数の値を取得したもので，エージェント数を100体，それぞれのエピソード数を1000とし100000個のデータを取る。次に部屋割り進捗データとは適当に決定したエージェント１体が環境に対して行動する度に，環境から評価関数の値を取得したものである。エージェントが一度も行動していないエピソードの最初にも評価関数の値を取っておく。１エピソードあたり1000回行動し，初期値の値も取るので1001個のデータがあり，エージェント１体は1000エピソード学習するので1001000個のデータを取る。取得した値は全てExcelファイルに記入する。選択手法はボルツマン選択を用いた。ボルツマン選択とは最初はおおよそランダムに行動選択し，経験を積むにしたがってgreedyになっていく選択手法のことである。また，ステップに応じて０から１ずつ増加するtに対して温度関数は
\begin{eqnarray*}
T(t)=\frac{4}{log(t+1.1)}
\end{eqnarray*}
である。問題生成に関しては人間１～12に自分以外の人に対してそれぞれ等確率にランダムで「同室希望」，「同室拒否」，「どちらでもない」を割り当てた。
\chapter{実験結果}
\label{実験結果}
１エージェント目の学習進捗データを図\ref{学習進捗データ（バケツリレーアルゴリズム，１エージェント目）}に示す。
\image{学習進捗データ（バケツリレーアルゴリズム，１エージェント目）}{外部ファイル/学習進捗データ/バケツリレーアルゴリズム/１エージェント目.png}

図\ref{学習進捗データ（バケツリレーアルゴリズム，１エージェント目）}と同様のデータ100エージェント分で平均を取ったものを図\ref{学習進捗データ（バケツリレーアルゴリズム，平均）}に示す。
\image{学習進捗データ（バケツリレーアルゴリズム，平均）}{外部ファイル/学習進捗データ/バケツリレーアルゴリズム/平均.png}

１エピソード目・250エピソード目・500エピソード目・750エピソード目・1000エピソード目の部屋割り進捗データを，それぞれ図\ref{部屋割り進捗データ（バケツリレーアルゴリズム，１エピソード目）}・図\ref{部屋割り進捗データ（バケツリレーアルゴリズム，250エピソード目）}・図\ref{部屋割り進捗データ（バケツリレーアルゴリズム，500エピソード目）}・図\ref{部屋割り進捗データ（バケツリレーアルゴリズム，750エピソード目）}・図\ref{部屋割り進捗データ（バケツリレーアルゴリズム，1000エピソード目）}に示す。
\image{部屋割り進捗データ（バケツリレーアルゴリズム，１エピソード目）}{外部ファイル/部屋割り進捗データ/バケツリレーアルゴリズム/１エピソード目}
\image{部屋割り進捗データ（バケツリレーアルゴリズム，250エピソード目）}{外部ファイル/部屋割り進捗データ/バケツリレーアルゴリズム/250エピソード目}
\image{部屋割り進捗データ（バケツリレーアルゴリズム，500エピソード目）}{外部ファイル/部屋割り進捗データ/バケツリレーアルゴリズム/500エピソード目}
\image{部屋割り進捗データ（バケツリレーアルゴリズム，750エピソード目）}{外部ファイル/部屋割り進捗データ/バケツリレーアルゴリズム/750エピソード目}
\image{部屋割り進捗データ（バケツリレーアルゴリズム，1000エピソード目）}{外部ファイル/部屋割り進捗データ/バケツリレーアルゴリズム/1000エピソード目}
\chapter{考察}
図\ref{学習進捗データ（バケツリレーアルゴリズム，１エージェント目）}を見てもなんとなく幸福度の総和が上昇しているのが分かるが，図\ref{学習進捗データ（バケツリレーアルゴリズム，平均）}を見るとそれが更に明確になる。全員が全員に対して「同室希望」を
\section{他のアルゴリズムとの比較}
\subsection{Profit Sharing}
\label{Profit Sharingとの比較}
第\ref{実験結果}章と同様に，７つのグラフを図\ref{学習進捗データ（Profit Sharing，１エージェント目）}・図\ref{学習進捗データ（Profit Sharing，平均）}・図\ref{部屋割り進捗データ（Profit Sharing，１エピソード目）}・図\ref{部屋割り進捗データ（Profit Sharing，250エピソード目）}・図\ref{部屋割り進捗データ（Profit Sharing，500エピソード目）}・図\ref{部屋割り進捗データ（Profit Sharing，750エピソード目）}・図\ref{部屋割り進捗データ（Profit Sharing，1000エピソード目）}に示す。
\image{学習進捗データ（Profit Sharing，１エージェント目）}{外部ファイル/学習進捗データ/Profit-Sharing/１エージェント目.png}
\image{学習進捗データ（Profit Sharing，平均）}{外部ファイル/学習進捗データ/Profit-Sharing/平均.png}
\image{部屋割り進捗データ（Profit Sharing，１エピソード目）}{外部ファイル/部屋割り進捗データ/Profit-Sharing/１エピソード目}
\image{部屋割り進捗データ（Profit Sharing，250エピソード目）}{外部ファイル/部屋割り進捗データ/Profit-Sharing/250エピソード目}
\image{部屋割り進捗データ（Profit Sharing，500エピソード目）}{外部ファイル/部屋割り進捗データ/Profit-Sharing/500エピソード目}
\image{部屋割り進捗データ（Profit Sharing，750エピソード目）}{外部ファイル/部屋割り進捗データ/Profit-Sharing/750エピソード目}
\image{部屋割り進捗データ（Profit Sharing，1000エピソード目）}{外部ファイル/部屋割り進捗データ/Profit-Sharing/1000エピソード目}

２ステップ以上過去のルールに遡って一気に評価値を更新すること以外はバケツリレーアルゴリズムと同じ，ということもあって概ねデータの傾向は同じだが，学習進捗データに僅かな違いがある。\ref{学習進捗データ（Profit Sharing，平均）}・\ref{学習進捗データ（バケツリレーアルゴリズム，平均）}を見比べてもあまり違いは分からないが，１エージェントのみに着目した\ref{学習進捗データ（Profit Sharing，１エージェント目）}・\ref{学習進捗データ（バケツリレーアルゴリズム，１エージェント目）}を見比べると，\ref{学習進捗データ（Profit Sharing，１エージェント目）}の方がブレが大きいことが分かる。

このような差が生まれた原因は，経験を評価値に反映する速さだと考えられる。Profit Sharingの方が経験を評価値に反映するのが早いため，直前の少数回の経験に影響されやすく能力にブレが生じやすい。
\subsection{Q学習}
\label{Q学習との比較}
第\ref{実験結果}章・\ref{Profit Sharingとの比較}と同様に，７つのグラフを図\ref{学習進捗データ（Q学習，１エージェント目）}・図\ref{学習進捗データ（Q学習，平均）}・図\ref{部屋割り進捗データ（Q学習，１エピソード目）}・図\ref{部屋割り進捗データ（Q学習，250エピソード目）}・図\ref{部屋割り進捗データ（Q学習，500エピソード目）}・図\ref{部屋割り進捗データ（Q学習，750エピソード目）}・図\ref{部屋割り進捗データ（Q学習，1000エピソード目）}に示す。
\image{学習進捗データ（Q学習，１エージェント目）}{外部ファイル/学習進捗データ/Q学習/１エージェント目.png}
\image{学習進捗データ（Q学習，平均）}{外部ファイル/学習進捗データ/Q学習/平均.png}
\image{部屋割り進捗データ（Q学習，１エピソード目）}{外部ファイル/部屋割り進捗データ/Q学習/１エピソード目}
\image{部屋割り進捗データ（Q学習，250エピソード目）}{外部ファイル/部屋割り進捗データ/Q学習/250エピソード目}
\image{部屋割り進捗データ（Q学習，500エピソード目）}{外部ファイル/部屋割り進捗データ/Q学習/500エピソード目}
\image{部屋割り進捗データ（Q学習，750エピソード目）}{外部ファイル/部屋割り進捗データ/Q学習/750エピソード目}
\image{部屋割り進捗データ（Q学習，1000エピソード目）}{外部ファイル/部屋割り進捗データ/Q学習/1000エピソード目}

図\ref{学習進捗データ（バケツリレーアルゴリズム，平均）}と図\ref{学習進捗データ（Q学習，平均）}を見比べると図\ref{学習進捗データ（バケツリレーアルゴリズム，平均）}の方が最終的な幸福度の総和が大きく，しかも早い段階で幸福度の総和が上がりきっている。よって，バケツリレーアルゴリズムの方がQ学習より良い学習をしたと言える。

幸福度の総和が上がりきるまでのエピソード数に差ができた原因としては，報酬の扱い方の差が考えられる。バケツリレーアルゴリズムでは報酬をそのまま評価値に加えるが，Q学習では報酬に学習率をかけて減衰させた上で評価値に加える。そのため，バケツリレーアルゴリズムの方が評価値が速く変動し早い段階でそれ以上ほとんど学習しない状態になったと考えられる。

ほとんど学習をしていない１エピソード目の部屋割り進捗データはバケツリレーアルゴリズム（図\ref{部屋割り進捗データ（バケツリレーアルゴリズム，１エピソード目）}）・Q学習（図\ref{部屋割り進捗データ（Q学習，１エピソード目）}）ともに０を中心とした激しく振動する推移になっているが，250エピソード目以降ではバケツリレーアルゴリズムの方が高い幸福度総和を維持しているステップが多いように見受けられる。
\subsection{Sarsa}
第\ref{実験結果}章・\ref{Profit Sharingとの比較}・\ref{Q学習との比較}と同様に，７つのグラフを図\ref{学習進捗データ（Sarsa，１エージェント目）}・図\ref{学習進捗データ（Sarsa，平均）}・図\ref{部屋割り進捗データ（Sarsa，１エピソード目）}・図\ref{部屋割り進捗データ（Sarsa，250エピソード目）}・図\ref{部屋割り進捗データ（Sarsa，500エピソード目）}・図\ref{部屋割り進捗データ（Sarsa，750エピソード目）}・図\ref{部屋割り進捗データ（Sarsa，1000エピソード目）}に示す。
\image{学習進捗データ（Sarsa，１エージェント目）}{外部ファイル/学習進捗データ/Sarsa/１エージェント目.png}
\image{学習進捗データ（Sarsa，平均）}{外部ファイル/学習進捗データ/Sarsa/平均.png}
\image{部屋割り進捗データ（Sarsa，１エピソード目）}{外部ファイル/部屋割り進捗データ/Sarsa/１エピソード目}
\image{部屋割り進捗データ（Sarsa，250エピソード目）}{外部ファイル/部屋割り進捗データ/Sarsa/250エピソード目}
\image{部屋割り進捗データ（Sarsa，500エピソード目）}{外部ファイル/部屋割り進捗データ/Sarsa/500エピソード目}
\image{部屋割り進捗データ（Sarsa，750エピソード目）}{外部ファイル/部屋割り進捗データ/Sarsa/750エピソード目}
\image{部屋割り進捗データ（Sarsa，1000エピソード目）}{外部ファイル/部屋割り進捗データ/Sarsa/1000エピソード目}

これらのグラフの傾向は\ref{Q学習との比較}で示したグラフと非常によく似ている。Q学習とSarsaの違いは評価値を伝搬させるとき伝搬元をどのように決定するかのみであって報酬の扱い方は同様なので，\ref{Q学習との比較}で述べた考察はここでも適用できる。
\begin{thebibliography}{99}
\bibitem{環境} takechan-NITNC, "room-allocation-reinforcement-learning-environment", \url{https://github.com/takechan-NITNC/room-allocation-reinforcement-learning-environment}, 2023/12/12参照.
\end{thebibliography}
\end{document}