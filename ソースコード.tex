\documentclass[titlepage]{ltjsreport}
\usepackage{luatexja, graphicx, multirow, listings, url, hyperref, comment}
\makeatletter
\renewcommand{\abstractname}{アブストラクト}
\newcommand{\image}[2]{\begin{figure}[h]\vspace{1em}\begin{center}\includegraphics[width = .9\textwidth]{#2}\caption{#1}\label{#1}\end{center}\end{figure}}
\newenvironment{mytable}[2]{\begin{table}[h]\begin{center}\caption{#1}\label{#1}\begin{tabular}{#2}}{\end{tabular}\end{center}\end{table}}
\newcommand{\inputtextbox}[3][]{\vspace{1em}\lstset{language = #1}\lstinputlisting[caption = #2, label = #2]{#3}}
\lstnewenvironment{textbox}[2][]{\vspace{1em}\lstset{language = #1, label = #2, caption = #2}}{}
\renewcommand{\lstlistingname}{\figurename}
\makeatother
\lstset{language = , commentstyle = {\itshape}, stringstyle = {\ttfamily}, frame = {tblr}, breaklines = true, numbers = left, xleftmargin = 5.5em, framexleftmargin = 2.5em, numberstyle = {\scriptsize}, captionpos = b}
\begin{document}
\makeatletter
\let\c@lstlisting=\c@figure
\makeatother
\begin{titlepage}
\vspace*{1em}
\begin{center}
\begin{huge}
情報工学実験
\\
\vspace{1em}
に関する実験
\end{huge}
\end{center}
\vspace{6em}
\begin{flushright}
\begin{LARGE}
情報工学科　年　番
\\
\vspace{1em}
名前
\end{LARGE}
\end{flushright}
\vspace{4em}
\begin{flushleft}
\begin{Large}
\begin{tabular}{ll}
提出期限： & //（）:
\\
提出日： & //（）
\\
\end{tabular}
\\
\vspace{2em}
共同実験者：班
\\
\vspace{1em}
\begin{tabular}{ll}
出席番号 & 名前
\\
出席番号 & 名前
\\
\end{tabular}
\end{Large}
\end{flushleft}
\end{titlepage}
\begin{abstract}
\end{abstract}
\chapter{解決方法}
環境は\cite{環境}のEnvironmentクラスに実装されている。

環境は12人が４部屋へどのように割り当てられているかを保持しているが，エージェントはそれを直接観測する訳ではない。

エージェントが観測する状態や環境が受け取る行動を定義するために，「attended rooms」「movable people」という概念を導入する。「attended rooms」とは環境が４部屋のうち２部屋をランダムに選択したものである。「movable people」とはattended roomsから１人ずつ，計２人ランダムに選択したものである。attended roomsには順序の概念があり，それぞれattended room 1・attended room 2とする。movable peopleも同様に，attended room 1から選ばれたmovable person 1とattended room 2から選ばれたmovable person 2の２人からなる。attended rooms・movable peopleを抽選する処理はEnvironmentクラスの\#drawメソッドに実装されている。\#drawメソッドを図\ref{attended rooms・movable peopleの抽選}に示す。この中で使われているsnatchRandomElement関数は引数のSetからランダムに１つ要素を取り出し，その要素を元のSetから削除する関数である。attended rooms・movable peopleの抽選はエージェントが環境に対して行動する度に行われる。
\begin{textbox}{attended rooms・movable peopleの抽選}
#draw() {
	const snatchedRooms = new Set([...this.rooms]);
	this.#attendedRooms = [];
	for (let i = 0; i < this.#a; i++) {
		this.#attendedRooms.push(snatchRandomElement(snatchedRooms));
	}
	this.#movablePeople = [];
	for (const attendedRoom of this.#attendedRooms) {
		const snatchedMembers = new Set([...attendedRoom.members]);
		for (let i = 0; i < this.#m; i++) {
			this.#movablePeople.push(snatchRandomElement(snatchedMembers));
		}
	}
}
\end{textbox}

エージェントが環境に対して状態を要求したとき，環境はmovable personに対して「同室希望」とした人の人数，「同室拒否」とした人の人数をattended room単位で集計する。例えば，movable person 1に対して「同室希望」とした人がattended room 2に２人，「同室拒否」とした人がattended room 2に１人いる場合，attended room 2からmovable person 1に対する集計結果は「同室希望：２　同室拒否：１」となる。環境がエージェントに対して返す状態は以下の４つを２×２の２次元配列に並べたものとなる。
\begin{enumerate}
\item attended room 1からmovable person 1に対する集計結果
\item attended room 2からmovable person 1に対する集計結果
\item attended room 1からmovable person 2に対する集計結果
\item attended room 2からmovable person 2に対する集計結果
\end{enumerate}

環境がエージェントに状態を提供する処理はEnvironmentクラスのgetStateメソッドに実装されている。getStateメソッドを図\ref{状態の提供}に示す。この中で使われているImpressionStatクラスはattended roomからmovable personに対する集計結果を表し，「同室希望」とした人数，「同室拒否」とした人数をそれぞれnumber型でフィールドに持つ。
\begin{textbox}{状態の提供}
getState(): ImpressionStat[][] {
	return this.#movablePeople.map((movablePerson) => {
		return this.#attendedRooms.map(function (attendedRoom) {
			return new ImpressionStat(
			[...attendedRoom.members].filter(function (member) {
				return member.likedPeople.has(movablePerson);
			}).length,
			[...attendedRoom.members].filter(function (member) {
				return member.dislikedPeople.has(movablePerson);
			}).length,
			);
		});
	});
}
\end{textbox}

エージェントが環境に対して行える行動はmovable peopleどうしの入れ替え，あるいは「何もしない」である。\cite{環境}には行動を表すActionクラスが定義されていて，１または２を代入可能なnumber型フィールドを２つ持つ。１・２あるいは２・１を持つAction型インスタンスはmovable person 1とmovable person2の入れ替えを表し，１・１，あるいは２・２を持つインスタンスは「何もしない」を表す。「何もしない」という行動を行った場合も，先述したattended rooms・movable peopleの抽選は行われる。

エージェントが環境に対して行動を行ったとき，幸福度の総和が変化するときがある。行動に対して環境がエージェントに返す報酬は，行動後の幸福度総和から行動前の幸福度総和を引いたものである。
\chapter{実験結果}
\label{実験結果}
１エージェント目の学習進捗データを図\ref{学習進捗データ（バケツリレーアルゴリズム，１エージェント目）}に示す。
\image{学習進捗データ（バケツリレーアルゴリズム，１エージェント目）}{外部ファイル/学習進捗データ/バケツリレーアルゴリズム/１エージェント目.png}

図\ref{学習進捗データ（バケツリレーアルゴリズム，１エージェント目）}と同様のデータ100エージェント分で平均を取ったものを図\ref{学習進捗データ（バケツリレーアルゴリズム，平均）}に示す。
\image{学習進捗データ（バケツリレーアルゴリズム，平均）}{外部ファイル/学習進捗データ/バケツリレーアルゴリズム/平均.png}

１エピソード目・250エピソード目・500エピソード目・750エピソード目・1000エピソード目の部屋割り進捗データを，それぞれ図\ref{部屋割り進捗データ（バケツリレーアルゴリズム，１エピソード目）}・図\ref{部屋割り進捗データ（バケツリレーアルゴリズム，250エピソード目）}・図\ref{部屋割り進捗データ（バケツリレーアルゴリズム，500エピソード目）}・図\ref{部屋割り進捗データ（バケツリレーアルゴリズム，750エピソード目）}・図\ref{部屋割り進捗データ（バケツリレーアルゴリズム，1000エピソード目）}に示す。
\image{部屋割り進捗データ（バケツリレーアルゴリズム，１エピソード目）}{外部ファイル/部屋割り進捗データ/バケツリレーアルゴリズム/１エピソード目}
\image{部屋割り進捗データ（バケツリレーアルゴリズム，250エピソード目）}{外部ファイル/部屋割り進捗データ/バケツリレーアルゴリズム/250エピソード目}
\image{部屋割り進捗データ（バケツリレーアルゴリズム，500エピソード目）}{外部ファイル/部屋割り進捗データ/バケツリレーアルゴリズム/500エピソード目}
\image{部屋割り進捗データ（バケツリレーアルゴリズム，750エピソード目）}{外部ファイル/部屋割り進捗データ/バケツリレーアルゴリズム/750エピソード目}
\image{部屋割り進捗データ（バケツリレーアルゴリズム，1000エピソード目）}{外部ファイル/部屋割り進捗データ/バケツリレーアルゴリズム/1000エピソード目}
\chapter{考察}
\section{他のアルゴリズムとの比較}
\subsection{Profit Sharing}
\label{Profit Sharingとの比較}
第\ref{実験結果}章と同様に，７つのグラフを図\ref{学習進捗データ（Profit Sharing，１エージェント目）}・図\ref{学習進捗データ（Profit Sharing，平均）}・図\ref{部屋割り進捗データ（Profit Sharing，１エピソード目）}・図\ref{部屋割り進捗データ（Profit Sharing，250エピソード目）}・図\ref{部屋割り進捗データ（Profit Sharing，500エピソード目）}・図\ref{部屋割り進捗データ（Profit Sharing，750エピソード目）}・図\ref{部屋割り進捗データ（Profit Sharing，1000エピソード目）}に示す。
\image{学習進捗データ（Profit Sharing，１エージェント目）}{外部ファイル/学習進捗データ/Profit-Sharing/１エージェント目.png}
\image{学習進捗データ（Profit Sharing，平均）}{外部ファイル/学習進捗データ/Profit-Sharing/平均.png}
\image{部屋割り進捗データ（Profit Sharing，１エピソード目）}{外部ファイル/部屋割り進捗データ/Profit-Sharing/１エピソード目}
\image{部屋割り進捗データ（Profit Sharing，250エピソード目）}{外部ファイル/部屋割り進捗データ/Profit-Sharing/250エピソード目}
\image{部屋割り進捗データ（Profit Sharing，500エピソード目）}{外部ファイル/部屋割り進捗データ/Profit-Sharing/500エピソード目}
\image{部屋割り進捗データ（Profit Sharing，750エピソード目）}{外部ファイル/部屋割り進捗データ/Profit-Sharing/750エピソード目}
\image{部屋割り進捗データ（Profit Sharing，1000エピソード目）}{外部ファイル/部屋割り進捗データ/Profit-Sharing/1000エピソード目}

２ステップ以上過去のルールに遡って一気に評価値を更新すること以外はバケツリレーアルゴリズムと同じ，ということもあって概ねデータの傾向は同じだが，学習進捗データに僅かな違いがある。\ref{学習進捗データ（Profit Sharing，平均）}・\ref{学習進捗データ（バケツリレーアルゴリズム，平均）}を見比べてもあまり違いは分からないが，１エージェントのみに着目した\ref{学習進捗データ（Profit Sharing，１エージェント目）}・\ref{学習進捗データ（バケツリレーアルゴリズム，１エージェント目）}を見比べると，\ref{学習進捗データ（Profit Sharing，１エージェント目）}の方がブレが大きいことが分かる。

このような差が生まれた原因は，経験を評価値に反映する速さだと考えられる。Profit Sharingの方が経験を評価値に反映するのが早いため，直前の少数回の経験に影響されやすく能力にブレが生じやすい。
\subsection{Q学習}
\label{Q学習との比較}
第\ref{実験結果}章・\ref{Profit Sharingとの比較}と同様に，７つのグラフを図\ref{学習進捗データ（Q学習，１エージェント目）}・図\ref{学習進捗データ（Q学習，平均）}・図\ref{部屋割り進捗データ（Q学習，１エピソード目）}・図\ref{部屋割り進捗データ（Q学習，250エピソード目）}・図\ref{部屋割り進捗データ（Q学習，500エピソード目）}・図\ref{部屋割り進捗データ（Q学習，750エピソード目）}・図\ref{部屋割り進捗データ（Q学習，1000エピソード目）}に示す。
\image{学習進捗データ（Q学習，１エージェント目）}{外部ファイル/学習進捗データ/Q学習/１エージェント目.png}
\image{学習進捗データ（Q学習，平均）}{外部ファイル/学習進捗データ/Q学習/平均.png}
\image{部屋割り進捗データ（Q学習，１エピソード目）}{外部ファイル/部屋割り進捗データ/Q学習/１エピソード目}
\image{部屋割り進捗データ（Q学習，250エピソード目）}{外部ファイル/部屋割り進捗データ/Q学習/250エピソード目}
\image{部屋割り進捗データ（Q学習，500エピソード目）}{外部ファイル/部屋割り進捗データ/Q学習/500エピソード目}
\image{部屋割り進捗データ（Q学習，750エピソード目）}{外部ファイル/部屋割り進捗データ/Q学習/750エピソード目}
\image{部屋割り進捗データ（Q学習，1000エピソード目）}{外部ファイル/部屋割り進捗データ/Q学習/1000エピソード目}

図\ref{学習進捗データ（バケツリレーアルゴリズム，平均）}と図\ref{学習進捗データ（Q学習，平均）}を見比べると図\ref{学習進捗データ（バケツリレーアルゴリズム，平均）}の方が最終的な幸福度の総和が大きく，しかも早い段階で幸福度の総和が上がりきっている。よって，バケツリレーアルゴリズムの方がQ学習より良い学習をしたと言える。

幸福度の総和が上がりきるまでのエピソード数に差ができた原因としては，報酬の扱い方の差が考えられる。バケツリレーアルゴリズムでは報酬をそのまま評価値に加えるが，Q学習では報酬に学習率をかけて減衰させた上で評価値に加える。そのため，バケツリレーアルゴリズムの方が評価値が速く変動し早い段階でそれ以上ほとんど学習しない状態になったと考えられる。

ほとんど学習をしていない１エピソード目の部屋割り進捗データはバケツリレーアルゴリズム（図\ref{部屋割り進捗データ（バケツリレーアルゴリズム，１エピソード目）}）・Q学習（図\ref{部屋割り進捗データ（Q学習，１エピソード目）}）ともに０を中心とした激しく振動する推移になっているが，250エピソード目以降ではバケツリレーアルゴリズムの方が高い幸福度総和を維持しているステップが多いように見受けられる。
\subsection{Sarsa}
第\ref{実験結果}章・\ref{Profit Sharingとの比較}・\ref{Q学習との比較}と同様に，７つのグラフを図\ref{学習進捗データ（Sarsa，１エージェント目）}・図\ref{学習進捗データ（Sarsa，平均）}・図\ref{部屋割り進捗データ（Sarsa，１エピソード目）}・図\ref{部屋割り進捗データ（Sarsa，250エピソード目）}・図\ref{部屋割り進捗データ（Sarsa，500エピソード目）}・図\ref{部屋割り進捗データ（Sarsa，750エピソード目）}・図\ref{部屋割り進捗データ（Sarsa，1000エピソード目）}に示す。
\image{学習進捗データ（Sarsa，１エージェント目）}{外部ファイル/学習進捗データ/Sarsa/１エージェント目.png}
\image{学習進捗データ（Sarsa，平均）}{外部ファイル/学習進捗データ/Sarsa/平均.png}
\image{部屋割り進捗データ（Sarsa，１エピソード目）}{外部ファイル/部屋割り進捗データ/Sarsa/１エピソード目}
\image{部屋割り進捗データ（Sarsa，250エピソード目）}{外部ファイル/部屋割り進捗データ/Sarsa/250エピソード目}
\image{部屋割り進捗データ（Sarsa，500エピソード目）}{外部ファイル/部屋割り進捗データ/Sarsa/500エピソード目}
\image{部屋割り進捗データ（Sarsa，750エピソード目）}{外部ファイル/部屋割り進捗データ/Sarsa/750エピソード目}
\image{部屋割り進捗データ（Sarsa，1000エピソード目）}{外部ファイル/部屋割り進捗データ/Sarsa/1000エピソード目}

これらのグラフの傾向は\ref{Q学習との比較}で示したグラフと非常によく似ている。Q学習とSarsaの違いは評価値を伝搬させるとき伝搬元をどのように決定するかのみであって報酬の扱い方は同様なので，\ref{Q学習との比較}で述べた考察はここでも適用できる。
\begin{thebibliography}{99}
\bibitem{環境} takechan-NITNC, "room-allocation-reinforcement-learning-environment", \url{https://github.com/takechan-NITNC/room-allocation-reinforcement-learning-environment}, 2023/12/12参照.
\end{thebibliography}
\end{document}